{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3J42CAGw4jtJOIV5jAyRB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksekhar/SVHN-is-a-real-world-image/blob/master/SVHN-is-a-real-world-image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **DOMAIN**: Autonomous Vehicles\n",
        "\n",
        "• **CONTEXT**: A Recognising multi-digit numbers in photographs captured at street level is an important component of modern-day map\n",
        "making. A classic example of a corpus of such street-level photographs is Google’s Street View imagery composed of hundreds of millions\n",
        "of geo-located 360-degree panoramic images.\n",
        "The ability to automatically transcribe an address number from a geo-located patch of pixels and associate the transcribed number with a\n",
        "known street address helps pinpoint, with a high degree of accuracy, the location of the building it represents. More broadly, recognising\n",
        "numbers in photographs is a problem of interest to the optical character recognition community.\n",
        "While OCR on constrained domains like document processing is well studied, arbitrary multi-character text recognition in photographs is\n",
        "still highly challenging. This difficulty arises due to the wide variability in the visual appearance of text in the wild on account of a large\n",
        "range of fonts, colours, styles, orientations, and character arrangements.\n",
        "The recognition problem is further complicated by environmental factors such as lighting, shadows, specularity, and occlusions as well as\n",
        "by image acquisition factors such as resolution, motion, and focus blurs. In this project, we will use the dataset with images centred around\n",
        "a single digit (many of the images do contain some distractors at the sides). Although we are taking a sample of the data which is simpler,\n",
        "it is more complex than MNIST because of the distractors.\n",
        "\n",
        "• **DATA DESCRIPTION**: The SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with the\n",
        "minimal requirement on data formatting but comes from a significantly harder, unsolved, real-world problem (recognising digits and\n",
        "numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.\n",
        "\n",
        "Where the labels for each of this image are the prominent number in that image i.e. 2,6,7 and 4 respectively.\n"
      ],
      "metadata": {
        "id": "KysI7WQnDrHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders"
      ],
      "metadata": {
        "id": "rKJO4O8_JNF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing modules"
      ],
      "metadata": {
        "id": "ccQCVKvIa3TY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79GCfspUDtZg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn import model_selection\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "import warnings\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,BatchNormalization,Flatten\n",
        "import random\n",
        "from tensorflow.keras import regularizers, optimizers, backend\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import category_encoders as ce\n",
        "\n",
        "\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hellping Class and functions"
      ],
      "metadata": {
        "id": "uAyj4DRcBfnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_confusion_matrix(cf,\n",
        "                          group_names=None,\n",
        "                          categories='auto',\n",
        "                          count=True,\n",
        "                          percent=True,\n",
        "                          cbar=True,\n",
        "                          xyticks=True,\n",
        "                          xyplotlabels=True,\n",
        "                          sum_stats=True,\n",
        "                          figsize=None,\n",
        "                          cmap='Blues',\n",
        "                          title=None):\n",
        "    '''\n",
        "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
        "    Arguments\n",
        "    '''\n",
        "\n",
        "\n",
        "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
        "    blanks = ['' for i in range(cf.size)]\n",
        "\n",
        "    if group_names and len(group_names)==cf.size:\n",
        "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
        "    else:\n",
        "        group_labels = blanks\n",
        "\n",
        "    if count:\n",
        "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
        "    else:\n",
        "        group_counts = blanks\n",
        "\n",
        "    if percent:\n",
        "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
        "    else:\n",
        "        group_percentages = blanks\n",
        "\n",
        "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
        "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
        "\n",
        "\n",
        "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
        "    if sum_stats:\n",
        "        #Accuracy is sum of diagonal divided by total observations\n",
        "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
        "\n",
        "\n",
        "\n",
        "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
        "    if figsize==None:\n",
        "        #Get default figure size if not set\n",
        "        figsize = plt.rcParams.get('figure.figsize')\n",
        "\n",
        "    if xyticks==False:\n",
        "        #Do not show categories if xyticks is False\n",
        "        categories=False\n",
        "\n",
        "\n",
        "    # MAKE THE HEATMAP VISUALIZATION\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
        "\n",
        "\n",
        "    if title:\n",
        "        plt.title(title)"
      ],
      "metadata": {
        "id": "e7vDBaKx39ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_drive(fpath):\n",
        "  try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive/\", force_remount=True)\n",
        "    google_drive_prefix = \"/content/drive/My Drive\"\n",
        "    data_path = f\"{google_drive_prefix}/{fpath}\"\n",
        "    print(data_path)\n",
        "    return data_path\n",
        "  except ModuleNotFoundError:\n",
        "    data_prefix = f\"{fpath}\""
      ],
      "metadata": {
        "id": "_nKlZ7QKugDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_name):\n",
        "  print(f\"reding the file: {file_name}\")\n",
        "  try:\n",
        "    file_type = file_name.split('.')[1]\n",
        "    print(file_type)\n",
        "    if file_type == 'csv':\n",
        "      df = pd.read_csv(file_name)\n",
        "    else:\n",
        "      pass\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    print(\"data write\")\n",
        "    raise e"
      ],
      "metadata": {
        "id": "g5Io3uTZAZxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_sample_data(df,records):\n",
        "  if not(records):\n",
        "    records = 5\n",
        "  print(f\"verifying first {records} records and last {records} records\")\n",
        "  return pd.concat([df.head(records),df.tail(records)])"
      ],
      "metadata": {
        "id": "b1O9HLEpKQ9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_percentage_of_missing_values(df):\n",
        "  print(\"Checking for missing values and print percentage for each attribute\")\n",
        "  return (round(df.isnull().sum() / (df.isnull().count())*100))"
      ],
      "metadata": {
        "id": "pmojHZSPdCpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_distribution(data,variable_name):\n",
        "\n",
        "  target_variable = data[variable_name]\n",
        "  plt.hist(target_variable)\n",
        "  plt.xlabel(f\"{variable_name}\")\n",
        "  plt.ylabel(\"Frequency\")\n",
        "  plt.title(\"Distribution of Target Variable\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "t65dFX7wB4KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_tring_accuracy(history,t_lebel,v_level,y_level):\n",
        "\n",
        "  plt.figure(figsize=(5, 3))\n",
        "  plt.plot(history.history['accuracy'], color='b', label=t_lebel)\n",
        "  plt.plot(history.history['val_accuracy'], color='r', label=v_level)\n",
        "  plt.title(f\"{t_lebel} and {v_level}\")\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel(y_level)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "F3-e7sqzY9_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(images, labels, num_images=10):\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=num_images, figsize=(10, 1))\n",
        "\n",
        "    for ax, image, label in zip(axes, images[:num_images], labels[:num_images]):\n",
        "        ax.set_axis_off()\n",
        "        ax.imshow(image, cmap=\"gray_r\", interpolation=\"nearest\")\n",
        "        ax.set_title(f\"Label: {label}\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6eiyZlQs_QnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data import and Understanding"
      ],
      "metadata": {
        "id": "41abRphQas4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = mount_drive(\"dataset/Autonomous_Vehicles_SVHN_single_grey1.h5\")"
      ],
      "metadata": {
        "id": "Ti7ADL-UvR3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***a.Read the .h5 file and assign to a variable.***"
      ],
      "metadata": {
        "id": "9Y12eGp5KHIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files  = h5py.File(file_path, 'r')"
      ],
      "metadata": {
        "id": "BIRmmQ6wl992"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Print all the keys from the .h5 file**"
      ],
      "metadata": {
        "id": "IFCpWQCxlal9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with files as f:\n",
        "  for key in f.keys():\n",
        "    print(key)"
      ],
      "metadata": {
        "id": "-I0HhcPKauP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. Split the data into X_train, X_test, Y_train, Y_test**"
      ],
      "metadata": {
        "id": "LCccHckOlj05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File(file_path, 'r') as f:\n",
        "\n",
        "    # Read all the datasets\n",
        "    X_test = f['X_test'][:]\n",
        "    X_train = f['X_train'][:]\n",
        "    y_test = f['y_test'][:]\n",
        "    y_train = f['y_train'][:]"
      ],
      "metadata": {
        "id": "-2TIuV7Hlf2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Visualisation and preprocessing."
      ],
      "metadata": {
        "id": "KKRcuRdClld_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Print shape of all the 4 data split into x, y, train, test to verify if x & y is in sync**"
      ],
      "metadata": {
        "id": "-_tADcrpnxlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shapes of all 4 variables\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Verify if train and test data is in sync\n",
        "if X_train.shape[0] == y_train.shape[0] and X_test.shape[0] == y_test.shape[0]:\n",
        "  print(\"Train and test data are in sync\")\n",
        "else:\n",
        "  print(\"Train and test data are not in sync\")"
      ],
      "metadata": {
        "id": "6GcXdUDeeJhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Visualise first 10 images in train data and print its corresponding labels. **: **bold text**"
      ],
      "metadata": {
        "id": "Ad5p4TAiATvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_images(X_train,y_train)"
      ],
      "metadata": {
        "id": "9mx0VwZs9vIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C.Reshape all the images with appropriate shape update the data in same variable.**"
      ],
      "metadata": {
        "id": "uqUl6W4G_uH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshape data from 2D to 1D -> 32X32 to 1024\n",
        "X_train = np.asarray(X_train).reshape(42000,1024)\n",
        "X_test = np.asarray(X_test).reshape(18000,1024)"
      ],
      "metadata": {
        "id": "sEqo3h8d_i-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D. Normalise the images i.e. Normalise the pixel values.**"
      ],
      "metadata": {
        "id": "didsb_2_QN2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "metadata": {
        "id": "3FWTTibG_m2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"Images in X_train:\", X_train.shape[0])\n",
        "print(\"Images in X_test:\", X_test.shape[0])\n",
        "print(\"Max value in X_train:\", X_train.max())\n",
        "print(\"Min value in X_train:\", X_train.min())"
      ],
      "metadata": {
        "id": "LuCrCZwJ_mnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E. Transform Labels into format acceptable by Neural Network**"
      ],
      "metadata": {
        "id": "8bKVH5ogbyc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"One value of y_train:\", y_train[0])"
      ],
      "metadata": {
        "id": "9zoblCh6_mY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F. Print total Number of classes in the Dataset**"
      ],
      "metadata": {
        "id": "XAJxyk7gcskd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of unique classes\n",
        "num_classes = len(np.unique(y_train))\n",
        "print(f\"Number of classes: {num_classes}\")"
      ],
      "metadata": {
        "id": "i8xpq8ipcvAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Training & Evaluation using Neural Network"
      ],
      "metadata": {
        "id": "axfJCoKYdIuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Design a Neural Network to train a classifier.**"
      ],
      "metadata": {
        "id": "-wHapGT0dLU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512,activation=\"relu\", kernel_initializer='he_normal',input_shape = (1024, )))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(256,activation=\"relu\", kernel_initializer='he_normal'))\n",
        "model.add(Dense(128,activation=\"relu\", kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(64,activation=\"relu\", kernel_initializer='he_normal'))\n",
        "model.add(Dense(32,activation=\"relu\", kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(16,activation=\"relu\", kernel_initializer='he_normal'))\n",
        "\n",
        "model.add(Dense(10,activation=\"softmax\"))\n"
      ],
      "metadata": {
        "id": "4STYfrWgdKOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Train the classifier using previously designed Architecture (Use best suitable parameters).**"
      ],
      "metadata": {
        "id": "bgcSv5LfdQsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(model_params, model):\n",
        "\n",
        "    X_train = model_params['X_train']\n",
        "    y_train = model_params['y_train']\n",
        "    X_test = model_params['X_test']\n",
        "    y_test = model_params['y_test']\n",
        "    optimizer = model_params['optimizer']\n",
        "    learning_rate = model_params.get('learning_rate', 0.0001)\n",
        "    batch_size = model_params.get('batch_size', 50)\n",
        "    epochs = model_params.get('epochs', 15)\n",
        "    patience = model_params.get('patience', 5)\n",
        "\n",
        "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    callbacks = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience)\n",
        "\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test,y_test), batch_size = batch_size, epochs = epochs, verbose = 1, callbacks=[callbacks])\n",
        "\n",
        "    accuracy_results = model.evaluate(X_test, y_test)\n",
        "\n",
        "    print('Test accuracy : ', round((accuracy_results[1]*100),2))\n",
        "\n",
        "    return history, accuracy_results\n"
      ],
      "metadata": {
        "id": "lLNhawyTB34W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model 1 adam optimizer with defind learning rate\n",
        "data = {\n",
        "    'X_train': X_train,\n",
        "    'y_train': y_train,\n",
        "    'X_test': X_test,\n",
        "    'y_test': y_test,\n",
        "    'optimizer': 'adam',\n",
        "    'learning_rate': 0.001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 10,\n",
        "    'patience': 3\n",
        "}\n",
        "\n",
        "history, accuracy_results = train_and_evaluate_model(data, model)"
      ],
      "metadata": {
        "id": "b18EhgaYEUCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "RN9BTkTD3pZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C. Evaluate performance of the model with appropriate metrics.**"
      ],
      "metadata": {
        "id": "HsHzmzmTdfQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "ND1BPEModgv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)"
      ],
      "metadata": {
        "id": "9EHmaGuG3vBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model was evaluated on 10 classes, labeled from 0 to 9. The performance metrics, including precision, recall, and F1-score, varied across the classes. Class 4 achieved the highest F1-score of 0.78, while class 6 had the lowest at 0.50.\n",
        "\n",
        "On average, the model correctly identified 78% of instances (precision), correctly retrieved 53% of instances (recall), and achieved a harmonic mean of precision and recall (F1-score) of 0.63. These averages were calculated in three ways: micro-average, macro-average, and weighted average. The micro-average does not consider class imbalance, while the weighted average does. The samples average, which is the average score for each instance, was 0.53 for all three metrics.\n",
        "\n",
        "In general, the model demonstrated higher precision than recall, indicating a greater accuracy but less comprehensiveness in its predictions. The significant variation in performance across different classes suggests potential benefits from further model tuning or using more balanced training data."
      ],
      "metadata": {
        "id": "m-yLuEPcX5Ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D. Plot the training loss, validation loss vs number of epochs and training accuracy, validation accuracy vs number of epochs plot and write your\n",
        "observations on the same.**"
      ],
      "metadata": {
        "id": "kbKQDIiCdhJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tring_accuracy(history,'Training Loss','Validation loss','loss')\n",
        "plot_tring_accuracy(history,'Training accuracy','Validation accuracy','accuracy')"
      ],
      "metadata": {
        "id": "3CNGQWKkd0-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First graph depicts the **Training Loss** and **Validation Loss** over a series of epochs. Here's a more elaborate breakdown:\n",
        "\n",
        "- **Training Loss**: The blue line represents the training loss, which quantifies how well the model fits the training data. As the model learns, the training loss decreases. Essentially, it measures the discrepancy between the predicted values and the actual values during training.\n",
        "\n",
        "- **Validation Loss**: The red line represents the validation loss, which assesses how well the model generalizes to unseen data (the validation set). A low validation loss indicates that the model performs well on new data. If the training loss decreases while the validation loss increases, it might indicate overfitting (where the model memorizes the training data but fails to generalize).\n",
        "\n",
        "- **Epochs**: The x-axis represents the number of training iterations (epochs). Each epoch corresponds to one complete pass through the entire training dataset. As the model trains, it adjusts its parameters to minimize the loss function.\n",
        "\n",
        "- **Convergence**: Around epoch 6, the training and validation losses start to converge. This convergence suggests that further training may not significantly improve the model's performance. It's essential to strike a balance between minimizing training loss and preventing overfitting.\n",
        "\n",
        "In summary, this graph illustrates the model's learning process: how it reduces loss over time and generalizes to new data.\n",
        "\n",
        "\n",
        "The second graph that represents the training and validation accuracy of a machine learning model over a series of epochs. It shows how the model's accuracy improves as it learns from the training data, and it also displays the accuracy as the model is validated against a separate set of data. The goal is to see both accuracies increase over time, indicating that the model is learning effectively and generalizing well to new data."
      ],
      "metadata": {
        "id": "WQXHCZ_EZm6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experement With different params"
      ],
      "metadata": {
        "id": "v7XDClXPJ3Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model 2 adam optimizer with defind learning rate with 50 epochs\n",
        "data = {\n",
        "    'X_train': X_train,\n",
        "    'y_train': y_train,\n",
        "    'X_test': X_test,\n",
        "    'y_test': y_test,\n",
        "    'optimizer': 'adam',\n",
        "    'learning_rate': 0.001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 50,\n",
        "    'patience': 3\n",
        "}\n",
        "\n",
        "history, accuracy_results = train_and_evaluate_model(data, model)\n",
        "\n",
        "y_pred=model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)\n",
        "\n",
        "\n",
        "plot_tring_accuracy(history,'Training Loss','Validation loss','loss')\n",
        "plot_tring_accuracy(history,'Training accuracy','Validation accuracy','accuracy')"
      ],
      "metadata": {
        "id": "ih2Hx9D8J75R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model 3 sgd optimizer with defind learning rate with 100 epochs\n",
        "data = {\n",
        "    'X_train': X_train,\n",
        "    'y_train': y_train,\n",
        "    'X_test': X_test,\n",
        "    'y_test': y_test,\n",
        "    'optimizer': 'sgd',\n",
        "    'learning_rate': 0.001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 50,\n",
        "    'patience': 3\n",
        "}\n",
        "\n",
        "history, accuracy_results = train_and_evaluate_model(data, model)\n",
        "\n",
        "y_pred=model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)\n",
        "\n",
        "\n",
        "plot_tring_accuracy(history,'Training Loss','Validation loss','loss')\n",
        "plot_tring_accuracy(history,'Training accuracy','Validation accuracy','accuracy')"
      ],
      "metadata": {
        "id": "7A6tgRkOKox2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**classification_report:**\n",
        "\n",
        "  The classification report shows the precision, recall, and F1-score for each class in a multi-class classification problem. Here's a summary of the performance metrics:\n",
        "\n",
        "- **Precision**: The proportion of true positive predictions among all positive predictions. It measures how accurate the positive predictions are.\n",
        "- **Recall**: The proportion of true positive predictions among all actual positive instances. It measures how well the model captures positive instances.\n",
        "- **F1-score**: The harmonic mean of precision and recall. It balances precision and recall, especially when there's an imbalance between classes.\n",
        "\n",
        "Overall, the model's performance seems reasonable, but it's essential to consider the specific context and requirements of your problem.  \n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "2oZPMuLCbIg-"
      }
    }
  ]
}